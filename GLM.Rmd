---
title: "Tutorial"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(mosaic)
sppcount <- read.csv("www/sppcount.csv")
knitr::opts_chunk$set(echo = FALSE)
```


## Generalised linear models

### Why might we want to generalise a linear model?
Recall your initial lectures, practicals and online websites from this module,
and remember that scientific data can arise in different 'forms' or 'types'. For
example:

* **continuous (ratio) data**
  * These could be positive or negative, and include decimal points, 2.45, 5.01,
  -3.78, 2.00. Examples might include temperate (degrees Centigrade).
* **continuous (ratio) data, bounded by zero**
  * Again your data have decimal points, but no zeros; 4.58, 2.912, 5.002 etc.
  Examples might include length (cm).
* **Whole number data, bounded by zero**
  * These are often count data, 4, 12, 0, 234. Examples might include the number
  of hedgehogs seen at a site.
* **Proportion or percentage data, 0 to 1, or 0 to 100**
  * These values can have decimals, but are bounded between 0 and 1 (or 0 and 100
  for percentages). Usually analysed as proportions. 0.23, 0.15, 0.98 etc. Can
  arise in lots of experiments, e.g. proportion of insects killed by a new pesticide
* **Binary categorical data**
  * Two categories present, often coded as 0 or 1. Again wide occurrence in 
  biology: dead/alive, present/absent, resistant/susceptible etc.
* **Ordinal categorical data**
  * Multiple categories present, 'first', 'second', 'third' etc. These can arise
  in questionnaire surveys where respondants have to select a preference from 1
  to 5, or river hydrology of stream networks.
  
### Baseline assumptions of linear model
A key assumption of your linear model, using the `lm()` function, is that the
**response variable** is continuous, i.e. could potentially have decimals. Strictly-
speaking, negative values should theoretically be possible, although in practice
for most datasets `lm()` is fairly robust to this assumption, and if your lower
bound is zero it generally does not cause too many problems. If you have continuous
data as your response, and you suspect your model is poorly fitted, you can
often easily correct this by taking a logarithm of your response variable. (Take
the logarithm as `log(response + 1)` if you have zero values.)

The statistical distribution underlying the standard `lm()` linear model is the
normal or "Gaussian" distribution, named after Friederick Gauss (German
mathematician). The graph below shows a frequency histogram of 100 random numbers,
with a mean of 50, and standard deviation of 5, generated using the `rnorm()`
function which uses a Gaussian or normal distribution.

```{r gaussian_distribution, echo=TRUE, cache=TRUE}
mydata <- rnorm(n = 100, mean = 50, sd = 5)
gf_histogram(~mydata) %>% 
  gf_lims(x = c(0, 100))
```

 Look at the scale of the x-axis and it is obvious that we have ratio data with
 decimal points, and we can confirm this by looking at the first few numbers in
 `mydata`:
 
```{r, echo=TRUE}
head(mydata)
```

As you can see from the above, there are two 'parameters' to a normal
distribution, the mean and the standard deviation. Conveniently, 95% of the
data will fall within + or - 1.96 (i.e. approximately 2) standard deviations.
Adjust the mean and sd here to see their effects.

```{r gaussian_distribution_ui, echo=FALSE}
    titlePanel("The normal (Gaussian) distribution")

    sidebarLayout(
        sidebarPanel(
            sliderInput("mean",
                        "Mean",
                        min = 0,
                        max = 100,
                        value = 50),
            sliderInput("sd",
                        "Standard deviation",
                        min = 0.1,
                        max = 10,
                        value = 5)
        ),

        mainPanel(
           plotOutput("gauss_plot")
        )
    )
```
```{r gaussian_distribution_server, context="server"}
output$gauss_plot <- renderPlot({
  gf_dhistogram(~ rnorm(n=100, mean=input$mean, sd=input$sd), bins = 20) %>% 
    gf_dist("norm", params = list(mean=input$mean, sd=input$sd), colour = "blue") %>% 
    gf_lims(x = c(0, 100), y = c(0, 0.15)) %>% 
    gf_labs(x = "Value of random normal variable",
            y = "Density (proportion of values)")
})
```

**Comments**

* Note how as you increase or decrease the mean, the distribution moves left or
right, but remains roughly the same shape
* A low standard deviation results in the values being packed together; a high
sd flattens and spreads the curve
* The blue line is a fitted curve according to the Gaussian distribution for 
these data.
 
## Poisson and Binomial distributions
The two distributions you are most likely to need as biologists are the Poisson
distribution for counts named after Simeon Poisson (French mathematician), and
the Binomial distribution for 1/0 or presence/absence data (no-one seems to have
claimed credit for this one!).

Let's have a quick look at these two distributions.

### Poisson distribution
Remember, this returns whole numbers (no decimals) greater than or equal to zero.
It takes a parameter $\lambda$ (Greek letter lambda) which adjusts the shape of
this distribution. Try the interactive below to see what happens:

```{r poisson_distribution_ui, echo=FALSE}
    titlePanel("The Poisson distribution for whole-number counts")

    sidebarLayout(
        sidebarPanel(
          withMathJax(),
            sliderInput("lambda",
                        "\\(\\lambda\\) (lambda)",
                        min = 1,
                        max = 100,
                        value = 50)
        ),

        mainPanel(
           plotOutput("poiss_plot")
        )
    )

```

```{r poisson_distribution_server, context="server"}
output$poiss_plot <- renderPlot({
    gf_dist("pois", params = list(lambda=input$lambda)) %>% 
    gf_lims(x = c(0, 100), y = c(0, 0.15)) %>% 
    gf_labs(x = "Value of random Poisson (count) variable",
            y = "Density (proportion of values)")
})
```

**Note**

* When $\lambda$ is high, you'll see that the Poisson distribution
looks fairly similar to a normal distribution, except with whole numbers. However
as $\lambda$ gets smaller, the distribution gets ***squashed*** to the left, as
it cannot have any values below zero.
* The histogram is shown slightly differently, as we are dealing with whole
number counts.

### Binomial distribution
This is useful when you have:

* dead / alive
* present / absent
* true / false
* proportion or percentage data

The main parameter is the probability of "success" on each "trial". So if you
toss a (fair) coin your probability of a head should be 0.5.  You might do an
experiment where you have 25 replicates, in each of which you try and germinate
10 seeds. The "size" of each replicate is 10, and you would have a count of
the numbers of seeds germinated or failed in each replicate.

Below, we have the simplest setup, 25 replicates, with each of size one (which
produces a very simple plot). Adjust the size of each replicate, which is
equivalent to the number of seeds per replicate in the example above, and the
probability of success, equivalent to the probability of germination.

```{r binomial_distribution_ui, echo=FALSE}
    titlePanel("The binomial distribution")

    sidebarLayout(
        sidebarPanel(
            sliderInput("size",
                        "Size of each trial (e.g. number of seeds per replicate)",
                        min = 1,
                        max = 50,
                        value = 1),
            sliderInput("prob",
                        "Probability of success (e.g. probability of seed germination)",
                        min = 0,
                        max = 1,
                        value = 0.5)
        ),

        mainPanel(
           plotOutput("binom_plot")
        )
    )

```

```{r binomial_distribution_server, context="server"}
output$binom_plot <- renderPlot({
    gf_histogram(~rbinom(n=25, size=input$size, prob=input$prob)) %>% 
    gf_lims(x = c(-2, input$size+2), y = c(-1, 25)) %>% 
    gf_labs(x = "Binomial count (e.g. number of seeds germinating per replicate)",
            y = "Number of replicates in this category")
})
```

**Note**

* Before you adjust the sliders, the sample size is only 1 (e.g. one seed per)
replicate, and the probability (of germination) is 0.5. Therefore roughly half
of your 25 replicates will fall into the 0 (no germination) category, and half
into the 1.0 (germination) category.
* As you increase the number of seeds per replicate (maximum of 50) you will
see additional whole numbers appearing on the horizontal axis, representing
the numbers of successes (e.g. germinating seeds)
* the total of all the vertical axes is always 25, which is the total number
of replicates in this example.

## How to generalise
### How do generalised linear models extend the standard linear model?
You have already come across the basic syntax for linear models of:

$$Response = \text{Explanatory variable(s)} + Error$$
where your explanatory variables are typically treatments, or environmental
measures, and your $Error$ is unknown variation or noise. A linear model assumes
that the $Error$ is normally distributed. This makes it relatively simple to
make a link from predictions calculated from your explanatory variable(s) to
your response.

Take a simple example, such as a 'regression' linear model, where you have a
continuous response variable $y$ and a continuous explanatory variable $x$. The
form of the model can then be expressed as:

$$y = a + b.x + \epsilon$$

Where

* $y$ = response variable, measured on a continuous scale (decimals allowed)
* $x$ = explanatory variable, in a regression this is continuous
* $a$ = intercept (where the fitted regression line crosses the vertical axis)
* $b$ = gradient of fitted line
* $\epsilon$ = unknown variation or noise in your data

When you create a linear model, the `lm()` function estimates the intercept and
slope parameters, to predict the response values as accurately as possible. The
above equation is often written as:

$$y = \beta_0 + \beta_1x + \epsilon$$
where

* $\beta_0$ = intercept
* $\beta_1$ = slope

This second style of writing has the advantage that it can be extended to multiple
explanatory variables, that might be continuous, and/or categorical.

$$y = \sum_{j=1}^{p} \beta_jx_j + \epsilon$$
where

* $p$ = the number of explanatory variables
* $j$ = a counter
* $\sum$ = Greek letter Epsilon which indicates to go through each of the next
calculations and add the results together
* $\beta_jx_j$ = multiply parameter. e.g. in the simple regression example we only
have two parameters ($p = 2$):  $\beta_0$ the intercept (which is multipled by
1.0) and $\beta_1$ the slope, which is multiplied by the explanatory variable $x_1$.

### The need for a link function
I mentioned in passing that we need to make our predictions and link them back
to the explanatory variable. This is straightforward if we have a normal (Gaussian)
distribution, but is complex for Binomial, Poisson and other distributions. 
Therefore in a generalised linear model or GLM, we calculate an **intermediate**
value, which is then "linked" back to the response variable. We call this
intermediate variable $\eta$ (Greek letter eta). So our equation now becomes:

$$\eta = \sum_{j=1}^{p} \beta_jx_j + \epsilon$$

### How to link back to the response
How to we then calculate
$$y = \eta$$
Fortunately, R's `glm()` function automatically picks the correct transformation
for the **link function**:

* normal (Gaussian) = do nothing (no change), or "identity" link.
* Poisson errors = log link. This uses natural logs, rather than logs to base 10
* Binomial errors = logit link. This is a little more complex, but R sets it
automatically

### Do I need to worry about this?
On a day-to-day basis, no (which is why I have avoided too many equations)! But
it does have a few impacts that you should be alert to, that we will discuss as
we work through examples:

* Least squares no longer used. The method for estimating the parameters changes
from sums of squares etc. to a technique called 'maximum likelihood'. Instead of
t-statistics for each parameter, you will see z-statistics displayed.
* You will no longer see an R-squared value. Instead, something called the
Akaike Information Criterion (AIC) will show for each `glm()` model. The **lower**
the AIC the better.
* Instead of sums of squares (SS) the `glm()` produces a **deviance** which can
be thought of as a 'generalised' SS.
* If making predictions for new data based on Poisson or Binomial errors, you
need an extra step to account for the link function.

## GLM with count data
### What goes wrong with `lm()`?
We'll begin by looking at a simple example of the numbers of species of aquatic
invertebrates in rivers with different levels of pollution. Aquatic invertebrates
can be very sensitive to water pollution, and are useful **bio-indicators** of
pollution events etc.

![](www/river.jpg)

An advanced system has now been developed by the Centre
for Ecology and Hydrology that is used by the Envionment Agency to assess river
quality, called  <a href="https://www.ceh.ac.uk/services/rivpacs-reference-database"
target="_blank">RIVPACS</a>. We will, however, just look at a simple set of species
counts and pollution.

The data are in a data.frame called `sppcount`. Begin by creating a scatterplot
of the data, run a linear model, examine its summary table, and look at the plots
of residuals and QQ plot. **Hints:** Use `gf_point()` for the scatterplot, for
the `lm()` make sure you have your response and explanatory variables the right
way round. Use `mplot(spp_count_lm)` to check all diagnostic plots or `gf_qq()`
with the model residuals.

```{r sppcount_lm, exercise=TRUE}

```
```{r sppcount_lm-solution}
# Create scatterplot
gf_point(species ~ pollution, data=sppcount)

# Create linear model, check coefficients and R-squared
sppcount_lm <- lm(species~pollution, data=sppcount)
summary(sppcount_lm)

# Display first two diagnostic plots
mplot(sppcount_lm, which=1:2)
```

At first site the linear model looks quite good, with a highly significant
negative effect of pollution on the count of species, and the model explains
almost 80% of the variation. However, there are two clues that something is
amiss:

1. The QQ plot has a distinct S-shape to it
2. What is the species count at high levels of pollution.

We can check the latter by plotting the fitted line onto our scatterplot, and
we can also calculate a predicted number of species for a given amount of water
pollution. As this is a **linear model** the line function does not do anything,
so we can use the simple `predict()` function to obtain a predicted spp count.

In the box below, create a scatterplot with the fitted line, and check the
number of species at a pollution level of about 15. You can extend the range of
your x-axis in your plot by adding the `gf_lim(x = c(0, 15))` option.

```{r sppcount_lm_predict, exercise=TRUE}
sppcount_lm <- lm(species ~ pollution, data = sppcount)


```
```{r sppcount_lm_predict-solution}
gf_point(species ~ pollution, data = sppcount) %>% 
  gf_lims(x = c(0, 15)) %>% 
  gf_lm()
```

We have a problem. From roughly pollution=12.5 onwards we are getting **negative**
numbers of species, which obviously cannot happen. To solve the problem, we'll
repeat the analysis, but as a generalised linear model.

### Correct analysis with `glm()` function
Remember that for count data the correct statistical distribution is the **Poisson distribution**
So simply replace your original `lm()` function with `glm()`, adding the option
in the latter that `family = poisson`. Again display the model `summary()` and
diagnostic plots.

```{r sppcount_glm, exercise=TRUE}

```
```{r sppcount_glm-solution}
# Create scatterplot
gf_point(species ~ pollution, data=sppcount)

# Create linear model, check coefficients and R-squared
sppcount_glm <- glm(species~pollution, family = poisson, data=sppcount)
summary(sppcount_glm)

# Display first two diagnostic plots
mplot(sppcount_glm, which=1:2)
```

**Key points from output of `glm()` model**

* The summary table again shows pollution as significant. Instead of an R-squared
value there is an AIC. When comparing models the lower the AIC the better.
* The QQ plot has an S-shaped curve, but this is less than before: compare the
range of values on the y-axis of the QQ plot for the `lm()` and `glm()`

```{r lm_v_glm, echo=FALSE}
question("What happens if I run glm() without specifying the family?",
         answer("It uses a normal distribution estimated by least squares", 
                message = "No. It uses a normal (Gaussian) distribution, but
                estimates by maximum likelihood. The coefficients for the
                intercept and gradient will be the same, but AIC and deviance
                will show."),
         answer("It uses a binomial distribution estimated by maximum likelihood",
                message = "No. It defaults to a normal (Gaussian) distribution, but
                estimates by maximum likelihood."),
         answer("It uses a Poisson distribution estimated by maximum likelihood",
                message = "No. It defaults to a normal (Gaussian) distribution, but
                estimates by maximum likelihood."),
         answer("It uses a Gaussian distribution estimated by maximum likelihood",
                correct = TRUE, message = "Good. It uses a normal (Gaussian) distribution, but
                estimates by maximum likelihood. Whilst the coefficients for the
                intercept and gradient will be the same as lm(), in glm() AIC and deviance
                will show."),
         allow_retry = TRUE,
         random_answer_order = TRUE)
```

### Adding the correct fitted line
You'll recall that GLMs use **link** functions to connect the response data to
the explanatories, that account for the appropriate characteristics of the
response data. Where the response data are continuous and `lm()` can be used,
the link function does not do anything, hence it is known as the "identity".
However, for Posson (and binomial) distributions, we need to take this into 
account.

Poisson (count) distributions use the log-link. We could therefore take the 
antilog of our predictions to get back to the values for our fitted line. Since
the `glm()` uses natural logarithms, often shown as $ln$ or $log_e$, the antilog
is the exponent. Hence $ln(x)$ is $e^x$. R has the function `exp()` to take the
antilogarithm.

However, this is a little complex to implement, so we are going to use a slightly
easier method, based on the `mosaic` package. The `mosaic` package's default `plotModel()`
function will allow you to see the GLM model and original points:


```{r glm_plot, echo=TRUE}
sppcount_glm <- glm(species~pollution, family = "poisson", data=sppcount)
plotModel(sppcount_glm)
```

To see what happens when we extend the x-axis to 15.0 (which you recall gave
**negative** species counts with the `lm()` model, we can extend the limits
of the x-axis).

```{r glm_plot_extended, echo=TRUE}
gf_point(species~pollution, data=sppcount) %>% 
  gf_lims(x = c(0, 15)) %>% 
  gf_smooth(method="glm", method.args = list(family="poisson"), fullrange=TRUE, se=TRUE)
```

The above code is a little more complex. We use `gf_smooth()` to add the predicted
model curve, with arguments to explain that a GLM with Poisson distribution should
be used. The `fullrange=TRUE` forces the fitted line to go beyond the range of the data.
This **extrapolation** is **NOT** normally recommended (you can see the SE bars are
bigger at the ends of the line). However, I have done it here just to demonstrate
that the predicted number of species no longer drops to unrealistic negative values.
